{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../output/training_data/train.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_images(row):\n",
    "    if (row['neighbours'] == 0) & (int(row['len_of_boundry_inv']) == 0):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.apply(remove_white_images, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5287, 8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean white images\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5287, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition = (data['neighbours'] == 0) & (data['len_of_boundry_inv'].astype(int) == 0)\n",
    "filtered_df = data.loc[~condition]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['neighbours'] == 0) & (data['len_of_boundry_inv'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Category'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\notebooks\\category-model.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m image_file_names \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features \u001b[39m=\u001b[39m data[[\u001b[39m'\u001b[39m\u001b[39mlen_of_boundry_inv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtile_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdisjoint_image\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mCategory\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Category'"
     ]
    }
   ],
   "source": [
    "# Separate image file names, features, and labels\n",
    "image_file_names = data['name'].values\n",
    "features = data[['len_of_boundry_inv','tile_size', 'disjoint_image']].values\n",
    "labels = data['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "image_train, image_val, features_train, features_val, labels_train, labels_val = train_test_split(\n",
    "    image_file_names, features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Category\" column to string type\n",
    "data['Category'] = data['Category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess black and white images\n",
    "image_data_train = []\n",
    "image_data_val = []\n",
    "\n",
    "for image_file in image_train:\n",
    "    # Load, preprocess, and resize the images\n",
    "    image = tf.keras.preprocessing.image.load_img(\n",
    "        '../output/tiles/' + image_file, target_size=(224, 224), color_mode='grayscale')\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image /= 255.0  # Normalize pixel values\n",
    "    image_data_train.append(image)\n",
    "\n",
    "for image_file in image_val:\n",
    "    # Load, preprocess, and resize the images\n",
    "    image = tf.keras.preprocessing.image.load_img(\n",
    "        '../output/tiles/' + image_file, target_size=(224, 224), color_mode='grayscale')\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image /= 255.0  # Normalize pixel values\n",
    "    image_data_val.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train shape: (272,)\n",
      "image_val shape: (69,)\n",
      "features_train shape: (272, 3)\n",
      "features_val shape: (69, 3)\n",
      "labels_train shape: (272,)\n",
      "labels_val shape: (69,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists to NumPy arrays\n",
    "# image_train, image_val, features_train, features_val, labels_train, labels_val \n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)\n",
    "# X_val = np.array(X_val)\n",
    "# y_val = np.array(y_val)\n",
    "# Check the shapes and dimensions of your data\n",
    "print(\"image_train shape:\", image_train.shape)\n",
    "print(\"image_val shape:\", image_val.shape)\n",
    "print(\"features_train shape:\", features_train.shape)\n",
    "print(\"features_val shape:\", features_val.shape)\n",
    "print(\"labels_train shape:\", labels_train.shape)\n",
    "print(\"labels_val shape:\", labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for black and white images\n",
    "image_data_train = np.asarray(image_data_train)\n",
    "image_data_val = np.asarray(image_data_val)\n",
    "\n",
    "features_train = np.asarray(features_train)\n",
    "features_val = np.asarray(features_val)\n",
    "\n",
    "labels_train = np.asarray(labels_train)\n",
    "labels_val = np.asarray(labels_val)\n",
    "\n",
    "image_datagen = ImageDataGenerator()\n",
    "image_train_gen = image_datagen.flow(image_data_train, labels_train, batch_size=32)\n",
    "image_val_gen = image_datagen.flow(image_data_val, labels_val, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the input shapes for your data match the actual data shapes\n",
    "image_shape = (224, 224, 1)\n",
    "feature_shape = (features.shape[1],)  # Adapt based on your feature data shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model to handle both types of data\n",
    "input_image = keras.layers.Input(shape=image_shape)\n",
    "input_features = keras.layers.Input(shape=feature_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate convolutional and dense layers for both inputs\n",
    "conv_layer = keras.layers.Conv2D(32, (3, 3))(input_image)\n",
    "max_pool = keras.layers.MaxPooling2D()(conv_layer)\n",
    "flatten = keras.layers.Flatten()(max_pool)\n",
    "dense_image = keras.layers.Dense(128)(flatten)\n",
    "\n",
    "dense_features = keras.layers.Dense(64)(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two pathways\n",
    "merged = keras.layers.concatenate([dense_image, dense_features])\n",
    "output = keras.layers.Dense(3, activation='softmax')(merged)  # Adjust 'your_num_classes'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = keras.models.Model(inputs=[input_image, input_features], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train type: <class 'keras.src.preprocessing.image.NumpyArrayIterator'>\n",
      "image_val type: <class 'keras.src.preprocessing.image.NumpyArrayIterator'>\n",
      "features_train type: <class 'numpy.ndarray'>\n",
      "features_val type: <class 'numpy.ndarray'>\n",
      "labels_train type: <class 'numpy.ndarray'>\n",
      "labels_val type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"image_train type:\", type(image_train_gen))\n",
    "print(\"image_val type:\", type(image_val_gen))\n",
    "print(\"features_train type:\", type(features_train))\n",
    "print(\"features_val type:\", type(features_val))\n",
    "print(\"labels_train type:\", type(labels_train))\n",
    "print(\"labels_val type:\", type(labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an image data generator\n",
    "image_datagen = ImageDataGenerator(rescale=1./255)\n",
    "image_train_gen = image_datagen.flow_from_directory(\n",
    "    '../output/tiles/',  # Path to your image data directory\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Adjust as needed\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\", \"<class 'keras.src.preprocessing.image.NumpyArrayIterator'>\"}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\notebooks\\category-model.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#model.fit([image_train_gen, features_train], labels_train, validation_data=([image_val_gen, features_val], labels_val), epochs=10)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit([image_train_gen, features_train], labels_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1102\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m         )\n\u001b[0;32m   1109\u001b[0m     )\n\u001b[0;32m   1110\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\", \"<class 'keras.src.preprocessing.image.NumpyArrayIterator'>\"}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#model.fit([image_train_gen, features_train], labels_train, validation_data=([image_val_gen, features_val], labels_val), epochs=10)\n",
    "model.fit([image_train_gen, features_train], labels_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\notebooks\\category-model.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Get the training history from the model's training\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#history = model.fit(train_gen, epochs=10)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Plot training & validation loss values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/rohit-ai-mesh-generation/ai-mesh-generation/notebooks/category-model.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32md:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\ROHITK~1\\AppData\\Local\\Temp\\3\\__autograph_generated_filebr1j3yxv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\rohit-ai-mesh-generation\\ai-mesh-generation\\venv\\lib\\site-packages\\keras\\src\\backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training history from the model's training\n",
    "#history = model.fit(train_gen, epochs=10)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate image file names, features, and labels\n",
    "# ['name','len_of_boundry','len_of_boundry_inv','tile_size', 'disjoint_image','num_triangles']\n",
    "image_file_names = data['name'].values\n",
    "features = data[['len_of_boundry_inv','tile_size', 'disjoint_image']].values\n",
    "labels = data['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess black and white images\n",
    "image_data = []\n",
    "for image_file in image_file_names:\n",
    "    # Load, preprocess, and resize the images\n",
    "    image = tf.keras.preprocessing.image.load_img('../output/tiles/' + image_file, target_size=(100, 100), color_mode='grayscale')\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image /= 255.0  # Normalize pixel values\n",
    "\n",
    "    image_data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2519b6cfdf0>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for black and white images\n",
    "image_data = np.array(image_data)\n",
    "image_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "image_train_gen = image_datagen.flow(image_data, labels, batch_size=32, subset='training')\n",
    "image_val_gen = image_datagen.flow(image_data, labels, batch_size=32, subset='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model to handle both types of data\n",
    "input_image = Input(shape=(224, 224, 1))\n",
    "input_features = Input(shape=(features.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Category\" column to string type\n",
    "data['Category'] = data['Category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
